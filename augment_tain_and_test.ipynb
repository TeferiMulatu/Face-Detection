{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea7aba9-3ff3-4901-af68-9de2f8cc27b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96fbabc9-1aa5-41b5-921d-789277b71e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 421 images in dataset\n",
      "[INFO] Using PIL augmentation method\n",
      "[INFO] Generating 22 augmented versions per image\n",
      "[INFO] Processed 5/421 images\n",
      "[INFO] Processed 10/421 images\n",
      "[INFO] Processed 15/421 images\n",
      "[INFO] Processed 20/421 images\n",
      "[INFO] Processed 25/421 images\n",
      "[INFO] Processed 30/421 images\n",
      "[INFO] Processed 35/421 images\n",
      "[INFO] Processed 40/421 images\n",
      "[INFO] Processed 45/421 images\n",
      "[INFO] Processed 50/421 images\n",
      "[INFO] Processed 55/421 images\n",
      "[INFO] Processed 60/421 images\n",
      "[INFO] Processed 65/421 images\n",
      "[INFO] Processed 70/421 images\n",
      "[INFO] Processed 75/421 images\n",
      "[INFO] Processed 80/421 images\n",
      "[INFO] Processed 85/421 images\n",
      "[INFO] Processed 90/421 images\n",
      "[INFO] Processed 95/421 images\n",
      "[INFO] Processed 100/421 images\n",
      "[INFO] Processed 105/421 images\n",
      "[INFO] Processed 110/421 images\n",
      "[INFO] Processed 115/421 images\n",
      "[INFO] Processed 120/421 images\n",
      "[INFO] Processed 125/421 images\n",
      "[INFO] Processed 130/421 images\n",
      "[INFO] Processed 135/421 images\n",
      "[INFO] Processed 140/421 images\n",
      "[INFO] Processed 145/421 images\n",
      "[INFO] Processed 150/421 images\n",
      "[INFO] Processed 155/421 images\n",
      "[INFO] Processed 160/421 images\n",
      "[INFO] Processed 165/421 images\n",
      "[INFO] Processed 170/421 images\n",
      "[INFO] Processed 175/421 images\n",
      "[INFO] Processed 180/421 images\n",
      "[INFO] Processed 185/421 images\n",
      "[INFO] Processed 190/421 images\n",
      "[INFO] Processed 195/421 images\n",
      "[INFO] Processed 200/421 images\n",
      "[INFO] Processed 205/421 images\n",
      "[INFO] Processed 210/421 images\n",
      "[INFO] Processed 215/421 images\n",
      "[INFO] Processed 220/421 images\n",
      "[INFO] Processed 225/421 images\n",
      "[INFO] Processed 230/421 images\n",
      "[INFO] Processed 235/421 images\n",
      "[INFO] Processed 240/421 images\n",
      "[INFO] Processed 245/421 images\n",
      "[INFO] Processed 250/421 images\n",
      "[INFO] Processed 255/421 images\n",
      "[INFO] Processed 260/421 images\n",
      "[INFO] Processed 265/421 images\n",
      "[INFO] Processed 270/421 images\n",
      "[INFO] Processed 275/421 images\n",
      "[INFO] Processed 280/421 images\n",
      "[INFO] Processed 285/421 images\n",
      "[INFO] Processed 290/421 images\n",
      "[INFO] Processed 295/421 images\n",
      "[INFO] Processed 300/421 images\n",
      "[INFO] Processed 305/421 images\n",
      "[INFO] Processed 310/421 images\n",
      "[INFO] Processed 315/421 images\n",
      "[INFO] Processed 320/421 images\n",
      "[INFO] Processed 325/421 images\n",
      "[INFO] Processed 330/421 images\n",
      "[INFO] Processed 335/421 images\n",
      "[INFO] Processed 340/421 images\n",
      "[INFO] Processed 345/421 images\n",
      "[INFO] Processed 350/421 images\n",
      "[INFO] Processed 355/421 images\n",
      "[INFO] Processed 360/421 images\n",
      "[INFO] Processed 365/421 images\n",
      "[INFO] Processed 370/421 images\n",
      "[INFO] Processed 375/421 images\n",
      "[INFO] Processed 380/421 images\n",
      "[INFO] Processed 385/421 images\n",
      "[INFO] Processed 390/421 images\n",
      "[INFO] Processed 395/421 images\n",
      "[INFO] Processed 400/421 images\n",
      "[INFO] Processed 405/421 images\n",
      "[INFO] Processed 410/421 images\n",
      "[INFO] Processed 415/421 images\n",
      "[INFO] Processed 420/421 images\n",
      "[INFO] Augmentation completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from imutils import paths\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image, ImageEnhance\n",
    "import random\n",
    "\n",
    "# Configuration\n",
    "dataset_path = \"C:/Users/hp/music/face_recognition_project/face_recognition_in_jupyter/Datasset/train\"\n",
    "output_path = \"C:/Users/hp/music/face_recognition_project/face_recognition_in_jupyter/Datasset/augmented\"\n",
    "augmentation_factor = 22  # Number of augmented images per original image\n",
    "image_size = (224, 224)  # Increased size for better quality\n",
    "use_pil_augmentation = True  # Set to False to use OpenCV/Keras method\n",
    "jpeg_quality = 95  # Quality for saved JPEGs (1-100)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "def high_quality_resize(image):\n",
    "    \"\"\"Resize image with high-quality interpolation\"\"\"\n",
    "    return cv2.resize(image, image_size, interpolation=cv2.INTER_LANCZOS4)\n",
    "\n",
    "def enhance_image_quality(image):\n",
    "    \"\"\"Apply quality enhancement to image\"\"\"\n",
    "    # CLAHE for contrast enhancement\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    l = clahe.apply(l)\n",
    "    lab = cv2.merge((l,a,b))\n",
    "    image = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "    \n",
    "    # Mild sharpening\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "    image = cv2.filter2D(image, -1, kernel)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def keras_augmentation(imagePaths):\n",
    "    \"\"\"Augmentation using Keras ImageDataGenerator\"\"\"\n",
    "    aug = ImageDataGenerator(\n",
    "        rotation_range=15,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode=\"reflect\",\n",
    "        brightness_range=[0.9, 1.1],\n",
    "        channel_shift_range=10.0,\n",
    "        rescale=1./255,\n",
    "        dtype='float32'\n",
    "    )\n",
    "\n",
    "    for (i, imagePath) in enumerate(imagePaths):\n",
    "        label = imagePath.split(os.path.sep)[-2]\n",
    "        output_dir = os.path.join(output_path, label)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Load and enhance original image\n",
    "        image = cv2.imread(imagePath)\n",
    "        image = enhance_image_quality(image)\n",
    "        image = high_quality_resize(image)\n",
    "        \n",
    "        # Save original\n",
    "        original_path = os.path.join(output_dir, f\"original_{i}.jpg\")\n",
    "        cv2.imwrite(original_path, image, [int(cv2.IMWRITE_JPEG_QUALITY), jpeg_quality])\n",
    "        \n",
    "        # Convert to array and expand dimensions for augmentation\n",
    "        image_array = np.expand_dims(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), axis=0)\n",
    "        \n",
    "        # Generate augmented images\n",
    "        total = 0\n",
    "        for batch in aug.flow(image_array, \n",
    "                             batch_size=1,\n",
    "                             save_to_dir=output_dir,\n",
    "                             save_prefix=f\"aug_{i}\",\n",
    "                             save_format=\"jpg\"):\n",
    "            total += 1\n",
    "            if total >= augmentation_factor:\n",
    "                break\n",
    "\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"[INFO] Processed {i + 1}/{len(imagePaths)} images\")\n",
    "\n",
    "def pil_augmentation(imagePaths):\n",
    "    \"\"\"Higher quality augmentation using PIL\"\"\"\n",
    "    for (i, imagePath) in enumerate(imagePaths):\n",
    "        label = imagePath.split(os.path.sep)[-2]\n",
    "        output_dir = os.path.join(output_path, label)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Open and resize original image\n",
    "        with Image.open(imagePath) as img:\n",
    "            # Convert to RGB if needed\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "                \n",
    "            # High-quality resize\n",
    "            img = img.resize(image_size, Image.LANCZOS)\n",
    "            \n",
    "            # Save original\n",
    "            original_path = os.path.join(output_dir, f\"original_{i}.jpg\")\n",
    "            img.save(original_path, quality=jpeg_quality)\n",
    "            \n",
    "            # Create augmented versions\n",
    "            for aug_num in range(augmentation_factor):\n",
    "                augmented = img.copy()\n",
    "                \n",
    "                # Random flip\n",
    "                if random.random() > 0.5:\n",
    "                    augmented = augmented.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                \n",
    "                # Random rotation (-15 to +15 degrees)\n",
    "                angle = random.uniform(-15, 15)\n",
    "                augmented = augmented.rotate(angle, resample=Image.BICUBIC, expand=False)\n",
    "                \n",
    "                # Random brightness and contrast\n",
    "                brightness = random.uniform(0.9, 1.1)\n",
    "                contrast = random.uniform(0.9, 1.1)\n",
    "                \n",
    "                enhancer = ImageEnhance.Brightness(augmented)\n",
    "                augmented = enhancer.enhance(brightness)\n",
    "                \n",
    "                enhancer = ImageEnhance.Contrast(augmented)\n",
    "                augmented = enhancer.enhance(contrast)\n",
    "                \n",
    "                # Random color balance adjustment\n",
    "                r, g, b = augmented.split()\n",
    "                r = r.point(lambda x: x * random.uniform(0.9, 1.1))\n",
    "                g = g.point(lambda x: x * random.uniform(0.9, 1.1))\n",
    "                b = b.point(lambda x: x * random.uniform(0.9, 1.1))\n",
    "                augmented = Image.merge('RGB', (r, g, b))\n",
    "                \n",
    "                # Save augmented image\n",
    "                aug_path = os.path.join(output_dir, f\"aug_{i}_{aug_num}.jpg\")\n",
    "                augmented.save(aug_path, quality=jpeg_quality)\n",
    "        \n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"[INFO] Processed {i + 1}/{len(imagePaths)} images\")\n",
    "\n",
    "def main():\n",
    "    imagePaths = sorted(list(paths.list_images(dataset_path)))\n",
    "    \n",
    "    print(f\"[INFO] Found {len(imagePaths)} images in dataset\")\n",
    "    print(f\"[INFO] Using {'PIL' if use_pil_augmentation else 'Keras'} augmentation method\")\n",
    "    print(f\"[INFO] Generating {augmentation_factor} augmented versions per image\")\n",
    "    \n",
    "    if use_pil_augmentation:\n",
    "        pil_augmentation(imagePaths)\n",
    "    else:\n",
    "        keras_augmentation(imagePaths)\n",
    "    \n",
    "    print(\"[INFO] Augmentation completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c246895-d6de-49d7-aeac-e6bb6253f2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "872a7cf2-5df9-4222-a354-d36de9c3d656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Output directory created at: C:/Users/hp/music/face_recognition_project/models/\n",
      "[INFO] Loading dataset from: C:/Users/hp/music/face_recognition_project/face_recognition_in_jupyter/Datasset/Augmented\n",
      "[INFO] Found 7 classes: ['Alemayehu' 'Banchiayehu' 'biniyam' 'martha' 'mikiyas' 'rahel' 'teferi']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ mobilenetv2_1.00_224 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,967</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ mobilenetv2_1.00_224 (\u001b[38;5;33mFunctional\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                │       \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)                   │           \u001b[38;5;34m8,967\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,266,951</span> (8.65 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,266,951\u001b[0m (8.65 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,967</span> (35.03 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,967\u001b[0m (35.03 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 170\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 170\u001b[0m         main()\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    172\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[ERROR] Training failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 155\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Training model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 155\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m    156\u001b[0m     X_train, y_train,\n\u001b[0;32m    157\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test),\n\u001b[0;32m    158\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mCONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    159\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mCONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    160\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    161\u001b[0m )\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# Save artifacts\u001b[39;00m\n\u001b[0;32m    164\u001b[0m save_artifacts(model, le, history)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:332\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    325\u001b[0m     (\n\u001b[0;32m    326\u001b[0m         val_x,\n\u001b[0;32m    327\u001b[0m         val_y,\n\u001b[0;32m    328\u001b[0m         val_sample_weight,\n\u001b[0;32m    329\u001b[0m     ) \u001b[38;5;241m=\u001b[39m data_adapter_utils\u001b[38;5;241m.\u001b[39munpack_x_y_sample_weight(validation_data)\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# Create an iterator that yields batches for one epoch.\u001b[39;00m\n\u001b[1;32m--> 332\u001b[0m epoch_iterator \u001b[38;5;241m=\u001b[39m TFEpochIterator(\n\u001b[0;32m    333\u001b[0m     x\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m    334\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    335\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    336\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m    337\u001b[0m     steps_per_epoch\u001b[38;5;241m=\u001b[39msteps_per_epoch,\n\u001b[0;32m    338\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39mshuffle,\n\u001b[0;32m    339\u001b[0m     class_weight\u001b[38;5;241m=\u001b[39mclass_weight,\n\u001b[0;32m    340\u001b[0m     distribute_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[0;32m    341\u001b[0m     steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution,\n\u001b[0;32m    342\u001b[0m )\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_symbolic_build(iterator\u001b[38;5;241m=\u001b[39mepoch_iterator)\n\u001b[0;32m    345\u001b[0m epoch_iterator\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:720\u001b[0m, in \u001b[0;36mTFEpochIterator.__init__\u001b[1;34m(self, distribute_strategy, *args, **kwargs)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy \u001b[38;5;241m=\u001b[39m distribute_strategy\n\u001b[1;32m--> 720\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_adapter\u001b[38;5;241m.\u001b[39mget_tf_dataset()\n\u001b[0;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedDataset):\n\u001b[0;32m    722\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy\u001b[38;5;241m.\u001b[39mexperimental_distribute_dataset(\n\u001b[0;32m    723\u001b[0m         dataset\n\u001b[0;32m    724\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_data_adapter.py:235\u001b[0m, in \u001b[0;36mArrayDataAdapter.get_tf_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    233\u001b[0m     indices_dataset \u001b[38;5;241m=\u001b[39m indices_dataset\u001b[38;5;241m.\u001b[39mmap(tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle)\n\u001b[1;32m--> 235\u001b[0m dataset \u001b[38;5;241m=\u001b[39m slice_inputs(indices_dataset, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inputs)\n\u001b[0;32m    237\u001b[0m options \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mOptions()\n\u001b[0;32m    238\u001b[0m options\u001b[38;5;241m.\u001b[39mexperimental_distribute\u001b[38;5;241m.\u001b[39mauto_shard_policy \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    239\u001b[0m     tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mAutoShardPolicy\u001b[38;5;241m.\u001b[39mDATA\n\u001b[0;32m    240\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_data_adapter.py:197\u001b[0m, in \u001b[0;36mArrayDataAdapter.get_tf_dataset.<locals>.slice_inputs\u001b[1;34m(indices_dataset, inputs)\u001b[0m\n\u001b[0;32m    191\u001b[0m inputs \u001b[38;5;241m=\u001b[39m array_slicing\u001b[38;5;241m.\u001b[39mconvert_to_sliceable(\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inputs, target_backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    193\u001b[0m )\n\u001b[0;32m    194\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mlists_to_tuples(inputs)\n\u001b[0;32m    196\u001b[0m dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mzip(\n\u001b[1;32m--> 197\u001b[0m     (indices_dataset, tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensors(inputs)\u001b[38;5;241m.\u001b[39mrepeat())\n\u001b[0;32m    198\u001b[0m )\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrab_batch\u001b[39m(i, data):\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrab_one\u001b[39m(x):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:741\u001b[0m, in \u001b[0;36mDatasetV2.from_tensors\u001b[1;34m(tensors, name)\u001b[0m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[39;00m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;66;03m# from_tensors_op -> dataset_ops).\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[0;32m    740\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_tensors_op\n\u001b[1;32m--> 741\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m from_tensors_op\u001b[38;5;241m.\u001b[39m_from_tensors(tensors, name)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_tensors_op.py:23\u001b[0m, in \u001b[0;36m_from_tensors\u001b[1;34m(tensors, name)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_tensors\u001b[39m(tensors, name):  \u001b[38;5;66;03m# pylint: disable=unused-private-name\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _TensorDataset(tensors, name)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_tensors_op.py:31\u001b[0m, in \u001b[0;36m_TensorDataset.__init__\u001b[1;34m(self, element, name)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, element, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     30\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"See `tf.data.Dataset.from_tensors` for details.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m   element \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mnormalize_element(element)\n\u001b[0;32m     32\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_structure \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mtype_spec_from_value(element)\n\u001b[0;32m     33\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_tensor_list(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_structure, element)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:134\u001b[0m, in \u001b[0;36mnormalize_element\u001b[1;34m(element, element_signature)\u001b[0m\n\u001b[0;32m    131\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    132\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(spec, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    133\u001b[0m         normalized_components\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 134\u001b[0m             ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(t, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomponent_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m i, dtype\u001b[38;5;241m=\u001b[39mdtype))\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mpack_sequence_as(pack_as, normalized_components)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:736\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;66;03m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[0;32m    735\u001b[0m preferred_dtype \u001b[38;5;241m=\u001b[39m preferred_dtype \u001b[38;5;129;01mor\u001b[39;00m dtype_hint\n\u001b[1;32m--> 736\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(\n\u001b[0;32m    737\u001b[0m     value, dtype, name, as_ref, preferred_dtype, accepted_result_types\n\u001b[0;32m    738\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[0;32m    225\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    226\u001b[0m           _add_error_prefix(\n\u001b[0;32m    227\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    231\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 234\u001b[0m   ret \u001b[38;5;241m=\u001b[39m conversion_func(value, dtype\u001b[38;5;241m=\u001b[39mdtype, name\u001b[38;5;241m=\u001b[39mname, as_ref\u001b[38;5;241m=\u001b[39mas_ref)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m    237\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_tensor_conversion.py:29\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constant_op  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[0;32m     28\u001b[0m _ \u001b[38;5;241m=\u001b[39m as_ref\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m constant_op\u001b[38;5;241m.\u001b[39mconstant(v, dtype\u001b[38;5;241m=\u001b[39mdtype, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:276\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(\n\u001b[0;32m    179\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    180\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ops\u001b[38;5;241m.\u001b[39mOperation, ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase]:\n\u001b[0;32m    181\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    277\u001b[0m                         allow_broadcast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:289\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    288\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m--> 289\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m    291\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39m_create_graph_constant(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    292\u001b[0m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[0;32m    293\u001b[0m )\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m const_tensor\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:301\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(\n\u001b[0;32m    298\u001b[0m     ctx, value, dtype, shape, verify_shape\n\u001b[0;32m    299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase:\n\u001b[0;32m    300\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 301\u001b[0m   t \u001b[38;5;241m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[0;32m    302\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:96\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to an `EagerTensor`.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03mNote that this function could return cached copies of created constants for\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m  TypeError: if `dtype` is not compatible with the type of t.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m     93\u001b[0m   \u001b[38;5;66;03m# Make a copy explicitly because the EagerTensor might share the underlying\u001b[39;00m\n\u001b[0;32m     94\u001b[0m   \u001b[38;5;66;03m# memory with the input array. Without this copy, users will be able to\u001b[39;00m\n\u001b[0;32m     95\u001b[0m   \u001b[38;5;66;03m# modify the EagerTensor after its creation by changing the input array.\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m   value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, ops\u001b[38;5;241m.\u001b[39mEagerTensor):\n\u001b[0;32m     98\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from imutils import paths\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"dataset_path\": \"C:/Users/hp/music/face_recognition_project/face_recognition_in_jupyter/Datasset/Augmented\",\n",
    "    \"output_dir\": \"C:/Users/hp/music/face_recognition_project/face_recognition_in_jupyter/\",\n",
    "    \"image_size\": (224, 224),\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 30,\n",
    "    \"test_size\": 0.2,\n",
    "    \"random_state\": 42\n",
    "}\n",
    "\n",
    "def create_output_dir(path):\n",
    "    \"\"\"Create output directory if it doesn't exist\"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    print(f\"[INFO] Output directory created at: {path}\")\n",
    "\n",
    "def load_dataset(dataset_path):\n",
    "    \"\"\"Load and preprocess dataset\"\"\"\n",
    "    print(f\"[INFO] Loading dataset from: {dataset_path}\")\n",
    "    imagePaths = list(paths.list_images(dataset_path))\n",
    "    \n",
    "    if not imagePaths:\n",
    "        raise FileNotFoundError(f\"No images found in {dataset_path}\")\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for imagePath in imagePaths:\n",
    "        try:\n",
    "            label = os.path.split(os.path.dirname(imagePath))[-1]\n",
    "            image = cv2.imread(imagePath)\n",
    "            \n",
    "            if image is None:\n",
    "                print(f\"[WARNING] Could not read image: {imagePath}\")\n",
    "                continue\n",
    "                \n",
    "            image = cv2.resize(image, CONFIG[\"image_size\"])\n",
    "            data.append(image)\n",
    "            labels.append(label)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Processing {imagePath}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not data:\n",
    "        raise ValueError(\"No valid images found in dataset\")\n",
    "    \n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "def build_model(num_classes):\n",
    "    \"\"\"Build the MobileNetV2 based model\"\"\"\n",
    "    base_model = MobileNetV2(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        input_shape=(*CONFIG[\"image_size\"], 3),\n",
    "        pooling=\"avg\"\n",
    "    )\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    inputs = Input(shape=(*CONFIG[\"image_size\"], 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def save_artifacts(model, le, history):\n",
    "    \"\"\"Save model, labels, and training history\"\"\"\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = os.path.join(CONFIG[\"output_dir\"], f\"model_{timestamp}\")\n",
    "    create_output_dir(output_dir)\n",
    "    \n",
    "    try:\n",
    "        # Save model\n",
    "        model_path = os.path.join(output_dir, \"face_recognition_model.h5\")\n",
    "        model.save(model_path)\n",
    "        print(f\"[SUCCESS] Model saved to: {model_path}\")\n",
    "        \n",
    "        # Save label encoder classes\n",
    "        classes_path = os.path.join(output_dir, \"classes.npy\")\n",
    "        np.save(classes_path, le.classes_)\n",
    "        print(f\"[SUCCESS] Classes saved to: {classes_path}\")\n",
    "        \n",
    "        # Save training history\n",
    "        history_path = os.path.join(output_dir, \"training_history.npy\")\n",
    "        np.save(history_path, history.history)\n",
    "        print(f\"[SUCCESS] Training history saved to: {history_path}\")\n",
    "        \n",
    "        # Save configuration\n",
    "        config_path = os.path.join(output_dir, \"config.txt\")\n",
    "        with open(config_path, 'w') as f:\n",
    "            for key, value in CONFIG.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "        print(f\"[SUCCESS] Config saved to: {config_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to save artifacts: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # Create output directory\n",
    "    create_output_dir(CONFIG[\"output_dir\"])\n",
    "    \n",
    "    # Load dataset\n",
    "    X, y = load_dataset(CONFIG[\"dataset_path\"])\n",
    "    \n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    num_classes = len(le.classes_)\n",
    "    y = to_categorical(y, num_classes=num_classes)\n",
    "    \n",
    "    print(f\"[INFO] Found {num_classes} classes: {le.classes_}\")\n",
    "    \n",
    "    # Split dataset\n",
    "    (X_train, X_test, y_train, y_test) = train_test_split(\n",
    "        X, y, \n",
    "        test_size=CONFIG[\"test_size\"], \n",
    "        stratify=y, \n",
    "        random_state=CONFIG[\"random_state\"]\n",
    "    )\n",
    "    \n",
    "    # Normalize pixel values\n",
    "    X_train = X_train.astype(\"float32\") / 255.0\n",
    "    X_test = X_test.astype(\"float32\") / 255.0\n",
    "    \n",
    "    # Build model\n",
    "    model = build_model(num_classes)\n",
    "    model.summary()\n",
    "    \n",
    "    # Train model\n",
    "    print(\"[INFO] Training model...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        batch_size=CONFIG[\"batch_size\"],\n",
    "        epochs=CONFIG[\"epochs\"],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Save artifacts\n",
    "    save_artifacts(model, le, history)\n",
    "    \n",
    "    print(\"[INFO] Training completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Training failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09a44d9-2ab7-4cb1-91bf-19fe06f8e094",
   "metadata": {},
   "outputs": [],
   "source": [
    "recognize image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20b03c52-2f18-4563-b372-8f752d2a81a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully. Recognizes 7 classes\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PIL import Image, ImageTk\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "\n",
    "class FaceRecognitionApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Face Recognition System\")\n",
    "        self.root.geometry(\"630x600\")  # Slightly taller for new elements\n",
    "        \n",
    "        # Configuration\n",
    "        self.model_input_size = (224, 224)  # Must match your trained model\n",
    "        self.display_size = (650, 360)      # Display resolution\n",
    "        self.confidence_threshold = 0.7     # Default confidence level\n",
    "        \n",
    "        # Load model with error handling\n",
    "        self.load_model_with_validation()\n",
    "        \n",
    "        # Initialize video\n",
    "        self.cap = cv2.VideoCapture(0)\n",
    "        self.video_running = False\n",
    "        self.live_recognition = False\n",
    "        self.current_frame = None\n",
    "        self.photo = None\n",
    "        \n",
    "        # Create GUI\n",
    "        self.create_widgets()\n",
    "        \n",
    "    def load_model_with_validation(self):\n",
    "        \"\"\"Safe model loading with validation\"\"\"\n",
    "        try:\n",
    "            self.model = load_model(\"face_recognition_model.h5\")\n",
    "            self.classes = np.load(\"classes.npy\")\n",
    "            print(f\"Model loaded successfully. Recognizes {len(self.classes)} classes\")\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Failed to load model: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            tk.messagebox.showerror(\"Error\", error_msg)\n",
    "            self.root.destroy()\n",
    "    \n",
    "    def create_widgets(self):\n",
    "        \"\"\"Build all GUI components\"\"\"\n",
    "        # Main container\n",
    "        self.main_frame = tk.Frame(self.root)\n",
    "        self.main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "        \n",
    "        # Video display\n",
    "        self.video_frame = tk.LabelFrame(self.main_frame, text=\"Camera Feed\")\n",
    "        self.video_frame.pack(pady=5)\n",
    "        \n",
    "        self.canvas = tk.Canvas(self.video_frame, width=self.display_size[0], \n",
    "                               height=self.display_size[1])\n",
    "        self.canvas.pack()\n",
    "        \n",
    "        # Control panel\n",
    "        self.control_frame = tk.LabelFrame(self.main_frame, text=\"Controls\")\n",
    "        self.control_frame.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        # Buttons\n",
    "        buttons = [\n",
    "            (\"Start Camera\", self.start_video),\n",
    "            (\"Stop Camera\", self.stop_video),\n",
    "            (\"Live Recognition\", self.toggle_live_recognition),\n",
    "            (\"Capture & Recognize\", self.capture_face),\n",
    "            (\"Upload Image\", self.upload_image)\n",
    "        ]\n",
    "        \n",
    "        for i, (text, command) in enumerate(buttons):\n",
    "            tk.Button(\n",
    "                self.control_frame, text=text, width=15, \n",
    "                command=command\n",
    "            ).grid(row=0, column=i, padx=5, pady=5)\n",
    "        \n",
    "        # Confidence threshold slider\n",
    "        self.threshold_slider = tk.Scale(\n",
    "            self.control_frame, \n",
    "            from_=0.5, to=0.95, \n",
    "            resolution=0.05,\n",
    "            orient=tk.HORIZONTAL,\n",
    "            label=\"Confidence Threshold\",\n",
    "            command=self.update_threshold\n",
    "        )\n",
    "        self.threshold_slider.set(self.confidence_threshold)\n",
    "        self.threshold_slider.grid(row=1, column=0, columnspan=5, sticky=\"ew\", padx=5)\n",
    "        \n",
    "        # Results display\n",
    "        self.result_frame = tk.LabelFrame(self.main_frame, text=\"Recognition Result\")\n",
    "        self.result_frame.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        self.result_label = tk.Label(\n",
    "            self.result_frame, text=\"No result yet\", font=(\"Arial\", 12))\n",
    "        self.result_label.pack(pady=10)\n",
    "        \n",
    "        # Status bar\n",
    "        self.status_bar = tk.Label(\n",
    "            self.main_frame, text=\"Ready\", \n",
    "            bd=1, relief=tk.SUNKEN, anchor=tk.W)\n",
    "        self.status_bar.pack(fill=tk.X)\n",
    "    \n",
    "    def update_threshold(self, value):\n",
    "        \"\"\"Update confidence threshold from slider\"\"\"\n",
    "        self.confidence_threshold = float(value)\n",
    "        self.status_bar.config(text=f\"Confidence threshold set to {value}\")\n",
    "    \n",
    "    def start_video(self):\n",
    "        \"\"\"Start video capture\"\"\"\n",
    "        if not self.video_running:\n",
    "            self.video_running = True\n",
    "            self.status_bar.config(text=\"Camera started\")\n",
    "            self.update_video()\n",
    "    \n",
    "    def stop_video(self):\n",
    "        \"\"\"Stop video capture\"\"\"\n",
    "        self.video_running = False\n",
    "        self.live_recognition = False\n",
    "        self.status_bar.config(text=\"Camera stopped\")\n",
    "    \n",
    "    def toggle_live_recognition(self):\n",
    "        \"\"\"Toggle real-time recognition\"\"\"\n",
    "        self.live_recognition = not self.live_recognition\n",
    "        if self.live_recognition:\n",
    "            if not self.video_running:\n",
    "                self.start_video()\n",
    "            self.status_bar.config(text=\"Live recognition enabled\")\n",
    "        else:\n",
    "            self.status_bar.config(text=\"Live recognition disabled\")\n",
    "    \n",
    "    def update_video(self):\n",
    "        \"\"\"Update video frame\"\"\"\n",
    "        if self.video_running:\n",
    "            try:\n",
    "                ret, frame = self.cap.read()\n",
    "                if not ret:\n",
    "                    self.status_bar.config(text=\"Error reading camera\")\n",
    "                    return\n",
    "                \n",
    "                # Process frame\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = cv2.resize(frame, self.display_size)\n",
    "                self.current_frame = frame.copy()\n",
    "                \n",
    "                # Apply recognition if enabled\n",
    "                if self.live_recognition:\n",
    "                    frame = self.process_frame(frame)\n",
    "                \n",
    "                # Display frame\n",
    "                img = Image.fromarray(frame)\n",
    "                self.photo = ImageTk.PhotoImage(image=img)\n",
    "                self.canvas.create_image(0, 0, anchor=tk.NW, image=self.photo)\n",
    "                \n",
    "                # Continue updating\n",
    "                self.root.after(30, self.update_video)\n",
    "            \n",
    "            except Exception as e:\n",
    "                self.status_bar.config(text=f\"Camera error: {str(e)}\")\n",
    "                self.stop_video()\n",
    "    \n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"Process frame for face recognition\"\"\"\n",
    "        display_frame = frame.copy()\n",
    "        \n",
    "        try:\n",
    "            # Prepare face for model\n",
    "            face = cv2.resize(frame, self.model_input_size)\n",
    "            face = face.astype(\"float32\") / 255.0\n",
    "            face = np.expand_dims(face, axis=0)\n",
    "            \n",
    "            # Predict\n",
    "            preds = self.model.predict(face, verbose=0)[0]\n",
    "            idx = np.argmax(preds)\n",
    "            proba = preds[idx]\n",
    "            name = self.classes[idx] if proba >= self.confidence_threshold else \"Unknown\"\n",
    "            \n",
    "            # Update UI\n",
    "            result_text = (f\"Recognized: {name} ({proba*100:.2f}%)\" \n",
    "                          if name != \"Unknown\" else \"Unknown face detected\")\n",
    "            self.result_label.config(text=result_text)\n",
    "            \n",
    "            # Annotate frame\n",
    "            cv2.putText(\n",
    "                display_frame, f\"{name}: {proba*100:.2f}%\",\n",
    "                (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, \n",
    "                (0, 255, 0) if name != \"Unknown\" else (0, 0, 255), 2\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.status_bar.config(text=f\"Recognition error: {str(e)}\")\n",
    "        \n",
    "        return display_frame\n",
    "    \n",
    "    def capture_face(self):\n",
    "        \"\"\"Capture and recognize current frame\"\"\"\n",
    "        if self.current_frame is None:\n",
    "            self.status_bar.config(text=\"No frame available to capture\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Process frame\n",
    "            face = cv2.resize(self.current_frame, self.model_input_size)\n",
    "            face = face.astype(\"float32\") / 255.0\n",
    "            face = np.expand_dims(face, axis=0)\n",
    "            \n",
    "            # Predict\n",
    "            preds = self.model.predict(face, verbose=0)[0]\n",
    "            idx = np.argmax(preds)\n",
    "            proba = preds[idx]\n",
    "            name = self.classes[idx] if proba >= self.confidence_threshold else \"Unknown\"\n",
    "            \n",
    "            # Show result\n",
    "            result_text = (f\"Recognized: {name} ({proba*100:.2f}%)\" \n",
    "                         if name != \"Unknown\" else \"Unknown face detected\")\n",
    "            self.result_label.config(text=result_text)\n",
    "            self.status_bar.config(text=\"Capture successful\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.result_label.config(text=\"Recognition failed\")\n",
    "            self.status_bar.config(text=f\"Error: {str(e)}\")\n",
    "    \n",
    "    def upload_image(self):\n",
    "        \"\"\"Upload and recognize image file\"\"\"\n",
    "        file_path = filedialog.askopenfilename(\n",
    "            filetypes=[(\"Image files\", \"*.jpg *.jpeg *.png *.bmp\")])\n",
    "        \n",
    "        if not file_path:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Read image\n",
    "            image = cv2.imread(file_path)\n",
    "            if image is None:\n",
    "                raise ValueError(\"Could not read image file\")\n",
    "            \n",
    "            # Convert and store\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            self.current_frame = image.copy()\n",
    "            \n",
    "            # Display with aspect ratio preservation\n",
    "            h, w = image.shape[:2]\n",
    "            aspect = w / h\n",
    "            new_w = min(self.display_size[0], int(self.display_size[1] * aspect))\n",
    "            new_h = min(self.display_size[1], int(self.display_size[0] / aspect))\n",
    "            resized = cv2.resize(image, (new_w, new_h))\n",
    "            \n",
    "            # Center on canvas\n",
    "            display_img = np.ones((self.display_size[1], self.display_size[0], 3), \n",
    "                                 dtype=np.uint8) * 255\n",
    "            x_offset = (self.display_size[0] - new_w) // 2\n",
    "            y_offset = (self.display_size[1] - new_h) // 2\n",
    "            display_img[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = resized\n",
    "            \n",
    "            # Show image\n",
    "            img = Image.fromarray(display_img)\n",
    "            self.photo = ImageTk.PhotoImage(image=img)\n",
    "            self.canvas.create_image(0, 0, anchor=tk.NW, image=self.photo)\n",
    "            \n",
    "            # Recognize\n",
    "            self.capture_face()\n",
    "            self.status_bar.config(text=f\"Loaded: {os.path.basename(file_path)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.result_label.config(text=\"Image load failed\")\n",
    "            self.status_bar.config(text=f\"Error: {str(e)}\")\n",
    "    \n",
    "    def on_closing(self):\n",
    "        \"\"\"Cleanup on window close\"\"\"\n",
    "        self.stop_video()\n",
    "        if hasattr(self, 'cap') and self.cap.isOpened():\n",
    "            self.cap.release()\n",
    "        if hasattr(self, 'photo'):\n",
    "            del self.photo\n",
    "        self.root.destroy()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = FaceRecognitionApp(root)\n",
    "    root.protocol(\"WM_DELETE_WINDOW\", app.on_closing)\n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc04cbc9-63d5-482e-aae6-cd270cbf1d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d71b273-4453-42e8-81f0-db96db09ba84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
